#Gradiant Descent와 Linear Regression에 대해 학습

#Gradiant Descent(경사하강법)은 기울기를 따라 점진적으로 파라미터를 업데이트하여 비용을 최소화

#학습률은 파라미터 업데이트의 보폭을 결정하는 요소

#Linear Regression은 데이터 점들 사이의 관계를 잘 나타내는 하나의 직선을 찾아내고 이를 통해 미래의 값을 예측하거나 데이터의 패턴을 이해하는 머신러닝 기법

#Batch gradiant Descent는 전체 데이터를 사용하여 업데이트해서 데이터가 많을 때 불리 + 시간이 오래 걸림

#Stochastic gradiant Descent는 하나의 학습데이터마다 파라미터를 업데이트하여 빠르게 학습

#데이터가 클때 스토캐스틱을 이용하고 시간이 지나면 학습률을 줄여서 보폭을 줄이는 방법을 채택하기도 함.

#경사하강법은 복잡한 행렬계산이 없어 대용량 데이터에 강하고 다양한 모델에 적용가능

#하지만, 학습률 결정을 위해 반복을 여러번 하므로 시간이 걸릴 수 있음.


#정규방정식(Normal Equation)은 단 한번의 수학 공식 계산으로 답을 찾음.

#반복이 없고 학습률 설정을 하지 않아도 됌.

#하지만, 특성의 개수가 1,000 ~ 10,000개를 넘어가면 경사 하강법보다 현저히 느려짐.

#역행렬이 존재하지 않을 수 있음(중복되는 경우가 포함됌)
