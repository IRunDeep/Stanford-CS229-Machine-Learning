#Naive Bayes의 한계점 지적: 훈련 데이터에 한 번도 등장하지 않은 단어가 테스트 데이터에 나타날 경우, 해당 단어의 확률이 0으로 계산되는 문제를 지적함. 

#위 문제를 해결하기 위해, 모든 단어의 카운트에 1과 같은 작은 수를 더해주는 Laplace Smoothing(또는 Additive Smoothing) 기법을 도입함. 이를 통해 어떤 단어도 확률이 0이 되는 것을 방지함.

#Multivariate Bernoulli Event Model: 이메일에 특정 단어의 '존재 여부'만을 이진(0 또는 1)으로 판단하는 모델임.

#Multinomial Event Model: 이메일에 특정 단어가 '몇 번 등장했는지' 빈도수까지 고려하는 모델임.

#실용적 조언: 머신러닝 프로젝트 초기에는 Naive Bayes처럼 간단하고 빠른 알고리즘으로 먼저 프로토타입을 제작하고, 오류 분석을 통해 점진적으로 성능을 개선해 나가는 '빠르고 더러운(quick and dirty)' 접근법의 효율성을 강조함.

#비선형적인 데이터 분류에 강점을 가진 Support Vector Machine(SVM)

