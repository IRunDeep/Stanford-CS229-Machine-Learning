#일반화 오류 (Generalization Error): 모델이 이전에 보지 못한 새로운 데이터에서 얼마나 잘 작동하는지를 나타냅니다. 기계 학습의 궁극적인 목표는 이 일반화 오류를 최소화하는 것입니다.

# 베이즈 오류 (환원 불가능한 오류): 데이터 자체에 내재된 노이즈로 인해 발생하는, 어떤 모델로도 줄일 수 없는 최소한의 오류입니다.

#추정 오류 (Estimation Error): 유한한 양의 훈련 데이터만을 사용하기 때문에 발생하는 오류입니다.

#호프딩 부등식 (Hoeffding's Inequality): 확률 변수의 합과 그 기댓값의 차이가 특정 값을 초과할 확률의 상한을 제공합니다. 이를 통해 경험적 오류와 실제 일반화 오류 사이의 관계를 분석할 수 있습니다.

#균등 수렴 (Uniform Convergence): 유한한 가설 클래스의 경우, 호프딩 부등식과 합집합 경계를 이용하여 훈련 샘플의 수가 충분히 많으면 경험적 위험이 일반화 위험에 가깝게 수렴한다는 것을 보일 수 있습니다. 이는 우리가 훈련 세트에서 좋은 성능을 얻으면, 새로운 데이터에서도 좋은 성능을 기대할 수 있다는 이론적 보장이 됩니다.

#VC 차원 (VC Dimension): 무한한 가설 클래스(예: SVM)의 복잡도를 측정하는 방법입니다. VC 차원은 해당 가설 클래스가 얼마나 다양한 패턴을 표현할 수 있는지를 나타내며, 이를 통해 무한 가설 클래스에 대해서도 일반화 오차의 한계를 유도할 수 있습니다.
