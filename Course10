#결정 트리는 데이터를 특정 규칙에 따라 영역으로 나누어 예측을 수행하는 비선형 모델입니다. 마치 '스무고개' 놀이와 같이, 연속된 질문을 통해 답을 찾아갑니다.

#오분류 손실(Misclassification Loss): 가장 직관적이지만, 클래스 분포 변화에 둔감하여 좋은 분할을 찾지 못할 수 있습니다.

#**교차 엔트로피(Cross-entropy)**와 지니 불순도(Gini Impurity): 더 민감하고 효과적인 손실 함수로, 자식 노드의 순도를 높이는 분할을 더 잘 찾아냅니다. 교차 엔트로피는 볼록(concave) 함수 형태를 띠어, 순도가 높은 자식 노드를 만들수록 평균 손실이 감소하는 특성이 있습니다.

# 여러 개의 모델(주로 약한 학습기)을 결합하여 단일 모델보다 훨씬 강력한 예측 성능을 만드는 기법입니다. 주로 모델의 분산(variance)을 줄이는 것이 목표입니다.  최대한 많은 모델(N)을 사용하되, 각 모델 간의 상관관계(ρ, Rho)는 최소화하는 것입니다.

# 배깅(Bagging)은 Bootstrap Aggregating의 줄임말로, 원본 훈련 데이터에서 중복을 허용하여 여러 개의 새로운 훈련 데이터셋(부트스트랩 샘플)을 만드는 방식입니다.

#효과: 각 모델이 서로 다른 데이터로 학습하므로 모델 간의 상관관계가 줄어들어 전체 모델의 분산이 감소합니다. 결정 트리는 분산이 높은 모델이라 배깅과 궁합이 매우 좋습니다.

#랜덤 포레스트 (Random Forests): 배깅을 결정 트리에 적용한 것에서 한 단계 더 나아간 모델입니다. 각 노드를 분할할 때마다 전체 특징 중 일부만 무작위로 선택하여 사용합니다. 이는 트리 간의 상관관계를 더욱 감소시켜 분산을 효과적으로 낮춥니다.

#**부스팅(Boosting)**은 배깅과 달리, 모델을 순차적으로 학습시키는 방법입니다. 주로 모델의 편향(bias)을 줄이는 것에 집중

#각 모델의 성능에 따라 다른 가중치(α)를 부여하여 최종 예측에 반영합니다. 성능이 좋은 모델의 의견을 더 존중하는 방식입니다.

#기본 학습기: 주로 결정 그루터기(Decision Stumps), 즉 깊이가 1인 매우 단순한 결정 트리를 사용합니다. 편향이 높은 약한 모델을 여러 개 연결하여 편향을 점차 줄여나가는 것이 부스팅의 핵심이기 때문입니다.
